{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acb557",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\nfrom collections import Counter\nimport math\nimport random\nfrom itertools import islice\n\n# Paths (adjust names if you used different filenames)\nGLOBAL_SNAP = Path(\"pcfg_all.json\")\nLONG_SNAP = Path(\"pcfg_of_len6_or_more.json\")\nFRAG_TSV = Path(\"frag_tokens_all.tsv\")   # FIXED: was frag_tokens_all_len_ge3.tsv\n\n# Load snapshots (prefer long snapshot for distributions; fall back to global)\ndef load_json(path):\n    if not path.exists():\n        return None\n    return json.loads(path.read_text(encoding=\"utf8\"))\n\nsnap_long = load_json(LONG_SNAP)\nsnap_global = load_json(GLOBAL_SNAP)\n\n# choose which snapshot to prefer for token frequencies\nsnap_pref = snap_long if snap_long else snap_global\nif not snap_pref:\n    raise RuntimeError(\"No snapshot JSON found: please ensure pcfg_of_len6_or_more.json or pcfg_all.json exist.\")\n\n# extract top templates, words, digits\ntop_templates = [tpl for tpl, cnt in snap_pref.get(\"top_templates\", [])]\ntop_words = [w for w, cnt in snap_pref.get(\"top_words\", [])]\ntop_digits = [d for d, cnt in snap_pref.get(\"top_digits\", [])]\n\n# Load frag counts from TSV (preferred) or fallback to snapshot FRAG if present\nfrag_counter = Counter()\nif FRAG_TSV.exists():\n    with FRAG_TSV.open(\"r\", encoding=\"utf8\") as f:\n        nxt = next(f)  # header\n        for line in f:\n            tok, cnt = line.rstrip(\"\\n\").split(\"\\t\")\n            frag_counter[tok] = int(cnt)\nelse:\n    # try JSON if you saved frag JSON earlier\n    frag_json = Path(\"frag_tokens_all_len_ge3.json\")\n    if frag_json.exists():\n        data = json.loads(frag_json.read_text(encoding=\"utf8\"))\n        for tok, cnt in data.get(\"frag_tokens\", []):\n            frag_counter[tok] = int(cnt)\n    else:\n        # no frag file â€” use FRAG from snapshot if present\n        if snap_pref.get(\"top_frags\"):\n            for tok, cnt in snap_pref[\"top_frags\"]:\n                frag_counter[tok] = int(cnt)\n\n# quick stats\nprint(f\"Using snapshot: {'long' if snap_long else 'global'}\")\nprint(f\"Loaded {len(top_templates)} templates, {len(top_words)} top words, {len(top_digits)} top digit runs, {len(frag_counter)} frag tokens\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c2fd6",
   "metadata": {},
   "outputs": [],
   "source": "# helper: parse template into slot list\ndef parse_template(tpl: str):\n    parts = tpl.split(\"|\")\n    slots = []\n    for part in parts:\n        if part.startswith(\"WORD\"):\n            n = int(part[4:]) if part[4:].isdigit() else None\n            slots.append((\"WORD\", n))\n        elif part.startswith(\"DIGITS\"):\n            n = int(part[6:]) if part[6:].isdigit() else None\n            slots.append((\"DIGITS\", n))\n        elif part == \"SYMBOL\":\n            slots.append((\"SYMBOL\", None))\n        elif part == \"FRAG\":\n            slots.append((\"FRAG\", None))\n        else:\n            # fallback: treat as FRAG\n            slots.append((\"FRAG\", None))\n    return slots\n\n# estimate minimal & maximal length possible for a template using top token lengths\ndef estimate_template_lengths(tpl, top_words_list, top_frags_list, top_digits_list, symbols_max=4):\n    slots = parse_template(tpl)\n    min_len = 0\n    # for FRAG use shortest observed top token; for WORD use their declared length (if present) or shortest top_words\n    for s, n in slots:\n        if s == \"DIGITS\":\n            if n: min_len += n\n            else:\n                min_len += (len(top_digits_list[0]) if top_digits_list else 1)\n        elif s == \"WORD\":\n            if n: min_len += n\n            else:\n                min_len += (len(top_words_list[0]) if top_words_list else 4)\n        elif s == \"SYMBOL\":\n            min_len += 1\n        elif s == \"FRAG\":\n            # use shortest frag (if any) otherwise assume 3\n            min_len += (len(top_frags_list[-1]) if top_frags_list else 3)\n    # maximal (heuristic): sum of max lengths of top lists (cap to avoid runaway)\n    max_len = 0\n    for s, n in slots:\n        if s == \"DIGITS\":\n            max_len += (len(top_digits_list[0]) if top_digits_list else 4)\n        elif s == \"WORD\":\n            max_len += (n if n else (len(top_words_list[0]) if top_words_list else 12))\n        elif s == \"SYMBOL\":\n            max_len += symbols_max\n        elif s == \"FRAG\":\n            max_len += (len(top_frags_list[0]) if top_frags_list else 12)\n    return min_len, max_len\n\n# prepare sorted frag and word lists (most->least frequent)\ntop_frags = [t for t,c in frag_counter.most_common()]\n# For efficiency, we'll use top-k lists (tuneable)\nTOP_WORDS_USE = 2000\nTOP_FRAGS_USE = 2000\nTOP_DIGITS_USE = 500\n\nwords_list = top_words[:TOP_WORDS_USE]\nfrags_list = top_frags[:TOP_FRAGS_USE]\ndigits_list = top_digits[:TOP_DIGITS_USE]\n\n# filter templates to those that can potentially reach length >= 6 (heuristic)\ndef templates_capable_of_len(templates, min_len=6):\n    capable = []\n    for tpl in templates:\n        minl, maxl = estimate_template_lengths(tpl, words_list, frags_list, digits_list)\n        if maxl >= min_len:\n            capable.append(tpl)\n    return capable\n\ntemplates_candidates = templates_capable_of_len(top_templates, min_len=6)\nprint(f\"{len(templates_candidates)} templates can potentially reach length >= 6 (from {len(top_templates)})\")\n# show top few\nfor t in templates_candidates[:10]:\n    print(\" \", t)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4c1f1",
   "metadata": {},
   "outputs": [],
   "source": "# Deterministic beam generator (pruned by partial log-score)\ndef slot_top_choices(slot_type, topk=200):\n    if slot_type == \"WORD\":\n        return words_list[:topk]\n    if slot_type == \"FRAG\":\n        return frags_list[:topk]\n    if slot_type == \"DIGITS\":\n        return digits_list[:topk]\n    if slot_type == \"SYMBOL\":\n        # simple symbol choices (common ones). You can expand if you recorded symbols in snapshot.\n        return [\"!\", \"@\", \"#\", \"$\", \"%\", \"&\", \"!!\", \"##\"]\n    return []\n\n# build simple frequency-based score maps from snapshots (fallback if no model.score available)\n# Use counts from snapshot if available\ndef build_count_map_from_snapshot(snapshot):\n    counts = {}\n    for w,c in snapshot.get(\"top_words\", []):\n        counts[(\"WORD\", w)] = c\n    for f,c in snapshot.get(\"top_digits\", []):\n        counts[(\"DIGITS\", f)] = c\n    # frags: use frag_counter\n    for f,c in frag_counter.items():\n        counts[(\"FRAG\", f)] = c\n    return counts\n\ncount_map = build_count_map_from_snapshot(snap_pref)\n\ndef partial_score(prefix_score, slot, token):\n    # a small additive log-score using counts (avoid zeros)\n    cnt = count_map.get((slot, token), 1)\n    return prefix_score + math.log(cnt + 1)\n\ndef generate_from_template_beam(tpl, topk_per_slot=200, prune_beam=2000, min_len=6, max_out_per_template=5000):\n    slots = parse_template(tpl)\n    # get candidate lists\n    slot_choices = [ slot_top_choices(s, topk_per_slot) for s,_ in slots ]\n    # beam will hold (partial_string, partial_score)\n    beam = [(\"\", 0.0)]\n    for i, (s, _n) in enumerate(slots):\n        new_beam = []\n        choices = slot_choices[i] or slot_top_choices(s, topk_per_slot)\n        for pref, pscore in beam:\n            for tok in choices:\n                cand = pref + tok\n                # prune if too long\n                if len(cand) > 64:\n                    continue\n                new_score = partial_score(pscore, s, tok)\n                new_beam.append((cand, new_score))\n        # keep top-K by partial score\n        new_beam.sort(key=lambda x: x[1], reverse=True)\n        beam = new_beam[:prune_beam]\n        if not beam:\n            break\n    # finalize: keep only those >= min_len, sort and yield\n    final = [(cand, score) for cand, score in beam if len(cand) >= min_len]\n    final.sort(key=lambda x: x[1], reverse=True)\n    for cand, score in final[:max_out_per_template]:\n        yield cand, score\n\n# Example: generate for first few templates and write to file\nOUT_DET = Path(\"candidates_det_ge6.txt\")\nOUT_DET.parent.mkdir(exist_ok=True)\nwritten = 0\nMAX_TOTAL = 200000  # tune: total candidates to write\nwith OUT_DET.open(\"w\", encoding=\"utf8\") as fout:\n    for tpl in templates_candidates[:40]:  # tune how many templates you try\n        for cand, sc in generate_from_template_beam(tpl, topk_per_slot=300, prune_beam=2000, min_len=6, max_out_per_template=2000):\n            fout.write(cand + \"\\n\")\n            written += 1\n            if written >= MAX_TOTAL:\n                break\n        if written >= MAX_TOTAL:\n            break\nprint(f\"Wrote {written} deterministic candidates to {OUT_DET.resolve()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a1ada",
   "metadata": {},
   "outputs": [],
   "source": "# stochastic sampler: sample from empirical frequency weights\ndef sample_from_counter(counter_list):\n    if not counter_list:  # safety check\n        return \"\"\n    tokens, counts = zip(*counter_list)\n    total = sum(counts)\n    weights = [c/total for c in counts]\n    return random.choices(tokens, weights=weights, k=1)[0]\n\n# prepare counters for sampling (use top lists)\nword_counter = [(w, next((c for (k,w2),c in count_map.items() if k==\"WORD\" and w2==w), 1)) for w in words_list]\ndigit_counter = [(d, next((c for (k,d2),c in count_map.items() if k==\"DIGITS\" and d2==d), 1)) for d in digits_list]\nfrag_counter_list = [(f, frag_counter[f]) for f in frags_list]\n\ndef stochastic_generate_for_template(tpl, n_samples=2000, min_len=6):\n    slots = parse_template(tpl)\n    results = {}\n    for _ in range(n_samples):\n        parts = []\n        for s, _ in slots:\n            if s == \"WORD\":\n                tok = sample_from_counter(word_counter)\n            elif s == \"FRAG\":\n                tok = sample_from_counter(frag_counter_list)\n            elif s == \"DIGITS\":\n                tok = sample_from_counter(digit_counter)\n            else:\n                tok = random.choice([\"!\", \"@\", \"#\"])\n            parts.append(tok)\n        cand = \"\".join(parts)\n        if len(cand) < min_len:\n            continue\n        # score approximate via product of counts (log)\n        sc = 0.0\n        for p, (s,_n) in zip(parts, slots):\n            sc += math.log(count_map.get((s, p), 1) + 1)\n        # keep best score per candidate\n        if cand not in results or results[cand] < sc:\n            results[cand] = sc\n    # return sorted\n    return sorted(results.items(), key=lambda x: x[1], reverse=True)\n\n# run sampler across top templates and write a file\nOUT_STO = Path(\"candidates_sto_ge6.txt\")\nwritten = 0\nwith OUT_STO.open(\"w\", encoding=\"utf8\") as fout:\n    for tpl in templates_candidates[:60]:\n        out = stochastic_generate_for_template(tpl, n_samples=3000, min_len=6)\n        for cand, sc in out[:1000]:\n            fout.write(cand + \"\\n\")\n            written += 1\nprint(f\"Wrote {written} stochastic candidates to {OUT_STO.resolve()}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83acb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot: long\n",
      "Loaded 1000 templates, 2000 top words, 500 top digit runs, 0 frag tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "from itertools import islice\n",
    "\n",
    "# Paths (adjust names if you used different filenames)\n",
    "GLOBAL_SNAP = Path(\"pcfg_all.json\")\n",
    "LONG_SNAP = Path(\"pcfg_of_len12_or_more.json\")\n",
    "FRAG_TSV = Path(\"frag_tokens_all_len_ge3.tsv\")   # or frag JSON if you prefer\n",
    "\n",
    "# Load snapshots (prefer long snapshot for distributions; fall back to global)\n",
    "def load_json(path):\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    return json.loads(path.read_text(encoding=\"utf8\"))\n",
    "\n",
    "snap_long = load_json(LONG_SNAP)\n",
    "snap_global = load_json(GLOBAL_SNAP)\n",
    "\n",
    "# choose which snapshot to prefer for token frequencies\n",
    "snap_pref = snap_long if snap_long else snap_global\n",
    "if not snap_pref:\n",
    "    raise RuntimeError(\"No snapshot JSON found: please ensure pcfg_snapshot_long_ge12.json or pcfg_snapshot_notebook.json exist.\")\n",
    "\n",
    "# extract top templates, words, digits\n",
    "top_templates = [tpl for tpl, cnt in snap_pref.get(\"top_templates\", [])]\n",
    "top_words = [w for w, cnt in snap_pref.get(\"top_words\", [])]\n",
    "top_digits = [d for d, cnt in snap_pref.get(\"top_digits\", [])]\n",
    "\n",
    "# Load frag counts from TSV (preferred) or fallback to snapshot FRAG if present\n",
    "frag_counter = Counter()\n",
    "if FRAG_TSV.exists():\n",
    "    with FRAG_TSV.open(\"r\", encoding=\"utf8\") as f:\n",
    "        nxt = next(f)  # header\n",
    "        for line in f:\n",
    "            tok, cnt = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            frag_counter[tok] = int(cnt)\n",
    "else:\n",
    "    # try JSON if you saved frag JSON earlier\n",
    "    frag_json = Path(\"frag_tokens_all_len_ge3.json\")\n",
    "    if frag_json.exists():\n",
    "        data = json.loads(frag_json.read_text(encoding=\"utf8\"))\n",
    "        for tok, cnt in data.get(\"frag_tokens\", []):\n",
    "            frag_counter[tok] = int(cnt)\n",
    "    else:\n",
    "        # no frag file â€” use FRAG from snapshot if present\n",
    "        if snap_pref.get(\"top_frags\"):\n",
    "            for tok, cnt in snap_pref[\"top_frags\"]:\n",
    "                frag_counter[tok] = int(cnt)\n",
    "\n",
    "# quick stats\n",
    "print(f\"Using snapshot: {'long' if snap_long else 'global'}\")\n",
    "print(f\"Loaded {len(top_templates)} templates, {len(top_words)} top words, {len(top_digits)} top digit runs, {len(frag_counter)} frag tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95c2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796 templates can potentially reach length >= 12 (from 1000)\n",
      "  FRAG\n",
      "  FRAG|DIGITS4\n",
      "  FRAG|DIGITS2\n",
      "  FRAG|DIGITS3\n",
      "  FRAG|DIGITS6\n",
      "  FRAG|DIGITS1\n",
      "  FRAG|SYMBOL|FRAG\n",
      "  FRAG|DIGITS8\n",
      "  FRAG|DIGITS5\n",
      "  FRAG|DIGITS1|FRAG\n"
     ]
    }
   ],
   "source": [
    "# helper: parse template into slot list\n",
    "def parse_template(tpl: str):\n",
    "    parts = tpl.split(\"|\")\n",
    "    slots = []\n",
    "    for part in parts:\n",
    "        if part.startswith(\"WORD\"):\n",
    "            n = int(part[4:]) if part[4:].isdigit() else None\n",
    "            slots.append((\"WORD\", n))\n",
    "        elif part.startswith(\"DIGITS\"):\n",
    "            n = int(part[6:]) if part[6:].isdigit() else None\n",
    "            slots.append((\"DIGITS\", n))\n",
    "        elif part == \"SYMBOL\":\n",
    "            slots.append((\"SYMBOL\", None))\n",
    "        elif part == \"FRAG\":\n",
    "            slots.append((\"FRAG\", None))\n",
    "        else:\n",
    "            # fallback: treat as FRAG\n",
    "            slots.append((\"FRAG\", None))\n",
    "    return slots\n",
    "\n",
    "# estimate minimal & maximal length possible for a template using top token lengths\n",
    "def estimate_template_lengths(tpl, top_words_list, top_frags_list, top_digits_list, symbols_max=4):\n",
    "    slots = parse_template(tpl)\n",
    "    min_len = 0\n",
    "    # for FRAG use shortest observed top token; for WORD use their declared length (if present) or shortest top_words\n",
    "    for s, n in slots:\n",
    "        if s == \"DIGITS\":\n",
    "            if n: min_len += n\n",
    "            else:\n",
    "                min_len += (len(top_digits_list[0]) if top_digits_list else 1)\n",
    "        elif s == \"WORD\":\n",
    "            if n: min_len += n\n",
    "            else:\n",
    "                min_len += (len(top_words_list[0]) if top_words_list else 4)\n",
    "        elif s == \"SYMBOL\":\n",
    "            min_len += 1\n",
    "        elif s == \"FRAG\":\n",
    "            # use shortest frag (if any) otherwise assume 3\n",
    "            min_len += (len(top_frags_list[-1]) if top_frags_list else 3)\n",
    "    # maximal (heuristic): sum of max lengths of top lists (cap to avoid runaway)\n",
    "    max_len = 0\n",
    "    for s, n in slots:\n",
    "        if s == \"DIGITS\":\n",
    "            max_len += (len(top_digits_list[0]) if top_digits_list else 4)\n",
    "        elif s == \"WORD\":\n",
    "            max_len += (n if n else (len(top_words_list[0]) if top_words_list else 12))\n",
    "        elif s == \"SYMBOL\":\n",
    "            max_len += symbols_max\n",
    "        elif s == \"FRAG\":\n",
    "            max_len += (len(top_frags_list[0]) if top_frags_list else 12)\n",
    "    return min_len, max_len\n",
    "\n",
    "# prepare sorted frag and word lists (most->least frequent)\n",
    "top_frags = [t for t,c in frag_counter.most_common()]\n",
    "# For efficiency, we'll use top-k lists (tuneable)\n",
    "TOP_WORDS_USE = 2000\n",
    "TOP_FRAGS_USE = 2000\n",
    "TOP_DIGITS_USE = 500\n",
    "\n",
    "words_list = top_words[:TOP_WORDS_USE]\n",
    "frags_list = top_frags[:TOP_FRAGS_USE]\n",
    "digits_list = top_digits[:TOP_DIGITS_USE]\n",
    "\n",
    "# filter templates to those that can potentially reach length >= 12 (heuristic)\n",
    "def templates_capable_of_len(templates, min_len=12):\n",
    "    capable = []\n",
    "    for tpl in templates:\n",
    "        minl, maxl = estimate_template_lengths(tpl, words_list, frags_list, digits_list)\n",
    "        if maxl >= min_len:\n",
    "            capable.append(tpl)\n",
    "    return capable\n",
    "\n",
    "templates_candidates = templates_capable_of_len(top_templates, min_len=12)\n",
    "print(f\"{len(templates_candidates)} templates can potentially reach length >= 12 (from {len(top_templates)})\")\n",
    "# show top few\n",
    "for t in templates_candidates[:10]:\n",
    "    print(\" \", t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec4c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 deterministic candidates to /Users/twochar/vS/Password-Decryption/candidates_det_ge12.txt\n"
     ]
    }
   ],
   "source": [
    "# Deterministic beam generator (pruned by partial log-score)\n",
    "def slot_top_choices(slot_type, topk=200):\n",
    "    if slot_type == \"WORD\":\n",
    "        return words_list[:topk]\n",
    "    if slot_type == \"FRAG\":\n",
    "        return frags_list[:topk]\n",
    "    if slot_type == \"DIGITS\":\n",
    "        return digits_list[:topk]\n",
    "    if slot_type == \"SYMBOL\":\n",
    "        # simple symbol choices (common ones). You can expand if you recorded symbols in snapshot.\n",
    "        return [\"!\", \"@\", \"#\", \"$\", \"%\", \"&\", \"!!\", \"##\"]\n",
    "    return []\n",
    "\n",
    "# build simple frequency-based score maps from snapshots (fallback if no model.score available)\n",
    "# Use counts from snapshot if available\n",
    "def build_count_map_from_snapshot(snapshot):\n",
    "    counts = {}\n",
    "    for w,c in snapshot.get(\"top_words\", []):\n",
    "        counts[(\"WORD\", w)] = c\n",
    "    for f,c in snapshot.get(\"top_digits\", []):\n",
    "        counts[(\"DIGITS\", f)] = c\n",
    "    # frags: use frag_counter\n",
    "    for f,c in frag_counter.items():\n",
    "        counts[(\"FRAG\", f)] = c\n",
    "    return counts\n",
    "\n",
    "count_map = build_count_map_from_snapshot(snap_pref)\n",
    "\n",
    "def partial_score(prefix_score, slot, token):\n",
    "    # a small additive log-score using counts (avoid zeros)\n",
    "    cnt = count_map.get((slot, token), 1)\n",
    "    return prefix_score + math.log(cnt + 1)\n",
    "\n",
    "def generate_from_template_beam(tpl, topk_per_slot=200, prune_beam=2000, min_len=12, max_out_per_template=5000):\n",
    "    slots = parse_template(tpl)\n",
    "    # get candidate lists\n",
    "    slot_choices = [ slot_top_choices(s, topk_per_slot) for s,_ in slots ]\n",
    "    # beam will hold (partial_string, partial_score)\n",
    "    beam = [(\"\", 0.0)]\n",
    "    for i, (s, _n) in enumerate(slots):\n",
    "        new_beam = []\n",
    "        choices = slot_choices[i] or slot_top_choices(s, topk_per_slot)\n",
    "        for pref, pscore in beam:\n",
    "            for tok in choices:\n",
    "                cand = pref + tok\n",
    "                # prune if too long\n",
    "                if len(cand) > 64:\n",
    "                    continue\n",
    "                new_score = partial_score(pscore, s, tok)\n",
    "                new_beam.append((cand, new_score))\n",
    "        # keep top-K by partial score\n",
    "        new_beam.sort(key=lambda x: x[1], reverse=True)\n",
    "        beam = new_beam[:prune_beam]\n",
    "        if not beam:\n",
    "            break\n",
    "    # finalize: keep only those >= min_len, sort and yield\n",
    "    final = [(cand, score) for cand, score in beam if len(cand) >= min_len]\n",
    "    final.sort(key=lambda x: x[1], reverse=True)\n",
    "    for cand, score in final[:max_out_per_template]:\n",
    "        yield cand, score\n",
    "\n",
    "# Example: generate for first few templates and write to file\n",
    "OUT_DET = Path(\"candidates_det_ge12.txt\")\n",
    "OUT_DET.parent.mkdir(exist_ok=True)\n",
    "written = 0\n",
    "MAX_TOTAL = 200000  # tune: total candidates to write\n",
    "with OUT_DET.open(\"w\", encoding=\"utf8\") as fout:\n",
    "    for tpl in templates_candidates[:40]:  # tune how many templates you try\n",
    "        for cand, sc in generate_from_template_beam(tpl, topk_per_slot=300, prune_beam=2000, min_len=12, max_out_per_template=2000):\n",
    "            fout.write(cand + \"\\n\")\n",
    "            written += 1\n",
    "            if written >= MAX_TOTAL:\n",
    "                break\n",
    "        if written >= MAX_TOTAL:\n",
    "            break\n",
    "print(f\"Wrote {written} deterministic candidates to {OUT_DET.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5a1ada",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OUT_STO.open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tpl \u001b[38;5;129;01min\u001b[39;00m templates_candidates[:\u001b[32m60\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         out = \u001b[43mstochastic_generate_for_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m cand, sc \u001b[38;5;129;01min\u001b[39;00m out[:\u001b[32m1000\u001b[39m]:\n\u001b[32m     48\u001b[39m             fout.write(cand + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mstochastic_generate_for_template\u001b[39m\u001b[34m(tpl, n_samples, min_len)\u001b[39m\n\u001b[32m     20\u001b[39m     tok = sample_from_counter(word_counter)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m s == \u001b[33m\"\u001b[39m\u001b[33mFRAG\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     tok = \u001b[43msample_from_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrag_counter_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m s == \u001b[33m\"\u001b[39m\u001b[33mDIGITS\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     24\u001b[39m     tok = sample_from_counter(digit_counter)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36msample_from_counter\u001b[39m\u001b[34m(counter_list)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_from_counter\u001b[39m(counter_list):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokens, counts = \u001b[38;5;28mzip\u001b[39m(*counter_list)\n\u001b[32m      4\u001b[39m     total = \u001b[38;5;28msum\u001b[39m(counts)\n\u001b[32m      5\u001b[39m     weights = [c/total \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m counts]\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# stochastic sampler: sample from empirical frequency weights\n",
    "def sample_from_counter(counter_list):\n",
    "    tokens, counts = zip(*counter_list)\n",
    "    total = sum(counts)\n",
    "    weights = [c/total for c in counts]\n",
    "    return random.choices(tokens, weights=weights, k=1)[0]\n",
    "\n",
    "# prepare counters for sampling (use top lists)\n",
    "word_counter = [(w, next((c for (k,w2),c in count_map.items() if k==\"WORD\" and w2==w), 1)) for w in words_list]\n",
    "digit_counter = [(d, next((c for (k,d2),c in count_map.items() if k==\"DIGITS\" and d2==d), 1)) for d in digits_list]\n",
    "frag_counter_list = [(f, frag_counter[f]) for f in frags_list]\n",
    "\n",
    "def stochastic_generate_for_template(tpl, n_samples=2000, min_len=12):\n",
    "    slots = parse_template(tpl)\n",
    "    results = {}\n",
    "    for _ in range(n_samples):\n",
    "        parts = []\n",
    "        for s, _ in slots:\n",
    "            if s == \"WORD\":\n",
    "                tok = sample_from_counter(word_counter)\n",
    "            elif s == \"FRAG\":\n",
    "                tok = sample_from_counter(frag_counter_list)\n",
    "            elif s == \"DIGITS\":\n",
    "                tok = sample_from_counter(digit_counter)\n",
    "            else:\n",
    "                tok = random.choice([\"!\", \"@\", \"#\"])\n",
    "            parts.append(tok)\n",
    "        cand = \"\".join(parts)\n",
    "        if len(cand) < min_len:\n",
    "            continue\n",
    "        # score approximate via product of counts (log)\n",
    "        sc = 0.0\n",
    "        for p, (s,_n) in zip(parts, slots):\n",
    "            sc += math.log(count_map.get((s, p), 1) + 1)\n",
    "        # keep best score per candidate\n",
    "        if cand not in results or results[cand] < sc:\n",
    "            results[cand] = sc\n",
    "    # return sorted\n",
    "    return sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# run sampler across top templates and write a file\n",
    "OUT_STO = Path(\"candidates_sto_ge12.txt\")\n",
    "written = 0\n",
    "with OUT_STO.open(\"w\", encoding=\"utf8\") as fout:\n",
    "    for tpl in templates_candidates[:60]:\n",
    "        out = stochastic_generate_for_template(tpl, n_samples=3000, min_len=12)\n",
    "        for cand, sc in out[:1000]:\n",
    "            fout.write(cand + \"\\n\")\n",
    "            written += 1\n",
    "print(f\"Wrote {written} stochastic candidates to {OUT_STO.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

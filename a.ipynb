{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3726a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce470ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/twochar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234377"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell A — Install and import NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download(\"words\")\n",
    "\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "# Build vocab set for fast lookup\n",
    "ENGLISH_VOCAB = set(w.lower() for w in nltk_words.words())\n",
    "len(ENGLISH_VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fa1d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Tokenizer + optional leet normalization helper\n",
    "\n",
    "LEET_MAP = str.maketrans({\n",
    "    '0': 'o', '1': 'l', '3': 'e', '4': 'a', '5': 's', '7': 't', '@': 'a', '$': 's', '!': 'i'\n",
    "})\n",
    "\n",
    "def leet_normalize(s: str) -> str:\n",
    "    \"\"\"Return a leet-normalized version of s (lowercased).\"\"\"\n",
    "    return s.translate(LEET_MAP).lower()\n",
    "\n",
    "def classify_run(r: str, do_leet: bool = False, use_vocab: bool = True):\n",
    "    \"\"\"\n",
    "    Classify a run (letters / digits / symbols) into (slot_type, token_for_counts, token_for_template).\n",
    "    - token_for_counts: normalized token used for slot_counts (lowercased, leet-normalized if do_leet)\n",
    "    - token_for_template: used to decide template text (WORD<N>, FRAG, DIGITS<N>, SYMBOL)\n",
    "    \"\"\"\n",
    "    if r.isdigit():\n",
    "        return \"DIGITS\", r, f\"DIGITS{len(r)}\"\n",
    "    if r.isalpha():\n",
    "        token_norm = r.lower()\n",
    "        if do_leet:\n",
    "            token_norm = leet_normalize(token_norm)\n",
    "        # decide WORD vs FRAG using vocab + length threshold\n",
    "        if use_vocab and len(token_norm) >= 3 and token_norm in ENGLISH_VOCAB:\n",
    "            return \"WORD\", token_norm, f\"WORD{len(token_norm)}\"\n",
    "        else:\n",
    "            return \"FRAG\", token_norm, \"FRAG\"\n",
    "    # else symbol / punctuation\n",
    "    return \"SYMBOL\", r, \"SYMBOL\"\n",
    "\n",
    "def tokenize(password: str, do_leet: bool = False, use_vocab: bool = True) -> Tuple[List[str], str]:\n",
    "    \"\"\"\n",
    "    Split password into runs and return (tokens_for_counts_list, template_str).\n",
    "    Template uses canonical slot descriptors (WORD<N>, DIGITS<N>, SYMBOL, FRAG).\n",
    "    The tokens returned are the normalized tokens used for counting (i.e. lowercase + leet-normalized if do_leet).\n",
    "    \"\"\"\n",
    "    pw = password.strip()\n",
    "    runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n",
    "    tokens_for_counts = []\n",
    "    template_parts = []\n",
    "    for r in runs:\n",
    "        slot, token_for_counts, tpl = classify_run(r, do_leet=do_leet, use_vocab=use_vocab)\n",
    "        tokens_for_counts.append(token_for_counts)\n",
    "        template_parts.append(tpl)\n",
    "    template = \"|\".join(template_parts)\n",
    "    return tokens_for_counts, template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c779fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCFGLite:\n",
    "    def __init__(self, alpha: float = 1.0, do_leet: bool = False):\n",
    "        self.template_counts = Counter()\n",
    "        self.slot_counts = defaultdict(Counter)  # slot_type -> Counter(token)\n",
    "        self.total_templates = 0\n",
    "        self.alpha = float(alpha)\n",
    "        self.do_leet = do_leet\n",
    "\n",
    "    def fit_list(self, pw_list, max_samples: int = None, verbose: bool = True, use_vocab: bool = True):\n",
    "        \"\"\"\n",
    "        Fit from an iterable/list of plaintext passwords.\n",
    "        If pw_list is an iterator/generator, it will be consumed streaming.\n",
    "        \"\"\"\n",
    "        for i, pw in enumerate(pw_list):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            if not pw:\n",
    "                continue\n",
    "            # get tokens and template using unified tokenize\n",
    "            tokens, template = tokenize(pw, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "            self.template_counts[template] += 1\n",
    "            self.total_templates += 1\n",
    "\n",
    "            # update slot counts by re-parsing runs so we keep the original run boundaries\n",
    "            runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n",
    "            for r in runs:\n",
    "                slot, token_for_counts, _ = classify_run(r, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "                self.slot_counts[slot][token_for_counts] += 1\n",
    "\n",
    "            # optional: progress print for very large datasets\n",
    "            if verbose and (i + 1) % 500000 == 0:\n",
    "                display(Markdown(f\"Trained on {i+1} passwords...\"))\n",
    "\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**Trained on {self.total_templates} templates. Unique templates: {len(self.template_counts)}**\"))\n",
    "\n",
    "    def fit_file(self, filepath: str, max_lines: int = None, use_vocab: bool = True):\n",
    "        p = Path(filepath)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(filepath)\n",
    "        # stream lines to avoid loading full file into memory\n",
    "        def iter_lines():\n",
    "            with p.open(\"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if max_lines and i >= max_lines:\n",
    "                        break\n",
    "                    yield line.rstrip(\"\\n\\r\")\n",
    "        self.fit_list(iter_lines(), max_samples=None, verbose=True, use_vocab=use_vocab)\n",
    "\n",
    "    def template_prob(self, template: str) -> float:\n",
    "        V = len(self.template_counts)\n",
    "        return (self.template_counts[template] + self.alpha) / (self.total_templates + self.alpha * (V + 1))\n",
    "\n",
    "    def slot_token_prob(self, slot_type: str, token: str) -> float:\n",
    "        counter = self.slot_counts.get(slot_type, Counter())\n",
    "        total = sum(counter.values())\n",
    "        V = len(counter)\n",
    "        return (counter[token] + self.alpha) / (total + self.alpha * (V + 1))\n",
    "\n",
    "    def score(self, password: str, use_vocab: bool = True) -> float:\n",
    "        \"\"\"Return natural-log probability score (higher = more likely under model).\"\"\"\n",
    "        # Use same tokenize/classify logic as training\n",
    "        tokens, template = tokenize(password, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "        logp = math.log(self.template_prob(template))\n",
    "        # Re-split runs to align with classification\n",
    "        runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', password)\n",
    "        for r in runs:\n",
    "            slot, token_for_counts, _ = classify_run(r, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "            p = self.slot_token_prob(slot, token_for_counts)\n",
    "            logp += math.log(p)\n",
    "        return logp\n",
    "\n",
    "    # keep the rest of your methods unchanged (top_templates/top_tokens/snapshot)\n",
    "    def top_templates(self, n=30):\n",
    "        return self.template_counts.most_common(n)\n",
    "\n",
    "    def top_tokens(self, slot_type: str, n=30):\n",
    "        return self.slot_counts.get(slot_type, Counter()).most_common(n)\n",
    "\n",
    "    def snapshot(self, top_templates_n=200, top_words_n=500, top_digits_n=200):\n",
    "        out = {\n",
    "            \"total_templates\": self.total_templates,\n",
    "            \"unique_templates\": len(self.template_counts),\n",
    "            \"top_templates\": self.top_templates(top_templates_n),\n",
    "            \"top_words\": self.top_tokens(\"WORD\", top_words_n),\n",
    "            \"top_digits\": self.top_tokens(\"DIGITS\", top_digits_n),\n",
    "        }\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8ff865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file: Data-Breach/rockyou.txt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 1000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 1500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 2000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 2500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 3000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 3500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 4000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 4500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 5000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 5500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 6000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 6500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 7000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 7500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 8000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 8500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 9000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 9500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 10000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 10500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 11000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 11500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 12000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 12500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 13000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 13500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 14000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Trained on 14344390 templates. Unique templates: 29295**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Top templates"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRAG                      4051390\n",
      "FRAG|DIGITS2              1440243\n",
      "FRAG|DIGITS1              886830\n",
      "FRAG|DIGITS4              755828\n",
      "FRAG|DIGITS3              515233\n",
      "DIGITS7                   487437\n",
      "DIGITS10                  478224\n",
      "DIGITS8                   428306\n",
      "DIGITS6                   390546\n",
      "DIGITS9                   307540\n",
      "FRAG|DIGITS6              212393\n",
      "FRAG|DIGITS5              128906\n",
      "WORD4|DIGITS4             125077\n",
      "FRAG|DIGITS1|FRAG         123929\n",
      "FRAG|SYMBOL               118006\n",
      "DIGITS11                  107864\n",
      "WORD6|DIGITS2             102431\n",
      "WORD5|DIGITS2             100013\n",
      "DIGITS1|FRAG              99932\n",
      "DIGITS4|FRAG              96901\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Top WORD tokens"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love                 23062\n",
      "ever                 18744\n",
      "life                 16089\n",
      "eva                  14720\n",
      "yahoo                9746\n",
      "baby                 8523\n",
      "may                  7791\n",
      "angel                7092\n",
      "sexy                 6318\n",
      "alex                 5420\n",
      "pink                 4977\n",
      "june                 4977\n",
      "sam                  4760\n",
      "jan                  4684\n",
      "girl                 4568\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Top DIGIT runs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          699853\n",
      "2          272123\n",
      "4          238996\n",
      "3          221212\n",
      "123        146979\n",
      "7          129316\n",
      "12         121734\n",
      "5          114328\n",
      "0          105195\n",
      "8          100934\n",
      "13         90319\n",
      "6          85355\n",
      "9          82820\n",
      "11         78797\n",
      "23         67325\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Demo run: use a small synthetic sample if you don't want to load rockyou now.\n",
    "# Configure DATA_PATH = \"/path/to/rockyou.txt\" to load real data. For demonstration we'll use a small list.\n",
    "\n",
    "DATA_PATH = \"Data-Breach/rockyou.txt\"  # <-- set to path string if you have the file accessible\n",
    "\n",
    "# Small synthetic sample (for quick demo)\n",
    "# sample_pw = [\n",
    "#     \"password\", \"123456\", \"qwerty\", \"letmein\", \"password1\", \"admin123\", \"iloveyou\", \"abc123\",\n",
    "#     \"sunshine\", \"passw0rd\", \"P@ssw0rd\", \"john1987\", \"alice2020!\", \"dragon\", \"welcome1\", \"football\"\n",
    "# ]\n",
    "\n",
    "model = PCFGLite(alpha=1.0, do_leet=True)\n",
    "\n",
    "if DATA_PATH:\n",
    "    print(\"Loading from file:\", DATA_PATH)\n",
    "    model.fit_file(DATA_PATH)  # change max_lines or remove it for full file\n",
    "else:\n",
    "    print(\"No DATA_PATH provided — running demo on synthetic sample.\")\n",
    "    # model.fit_list(sample_pw, max_samples=None)\n",
    "\n",
    "# show top templates and top tokens\n",
    "display(Markdown(\"### Top templates\"))\n",
    "for t, c in model.top_templates(20):\n",
    "    print(f\"{t:25} {c}\")\n",
    "\n",
    "display(Markdown(\"### Top WORD tokens\"))\n",
    "for w, c in model.top_tokens(\"WORD\", 15):\n",
    "    print(f\"{w:20} {c}\")\n",
    "\n",
    "display(Markdown(\"### Top DIGIT runs\"))\n",
    "for d, c in model.top_tokens(\"DIGITS\", 15):\n",
    "    print(f\"{d:10} {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad693c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f424a70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Example scores (higher = more likely under model)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "password         score = -14.1072\n",
      "P@ssw0rd         score = -42.6932\n",
      "john1987         score = -18.3355\n",
      "unique!X9        score = -34.2135\n",
      "iloveyou         score = -10.4604\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Scoring examples & usage\n",
    "examples = [\"password\", \"P@ssw0rd\", \"john1987\", \"unique!X9\", \"iloveyou\"]\n",
    "display(Markdown(\"### Example scores (higher = more likely under model)\"))\n",
    "for ex in examples:\n",
    "    print(f\"{ex:15}  score = {model.score(ex):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8114bf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Snapshot saved to **/Users/twochar/vS/Password-Decryption/pcfg_snapshot_notebook.json** — contains top templates and top tokens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6 — Save snapshot for inspection (JSON)\n",
    "snap = model.snapshot()\n",
    "out_path = Path(\"pcfg_snapshot_notebook.json\")\n",
    "out_path.write_text(json.dumps(snap))\n",
    "display(Markdown(f\"Snapshot saved to **{out_path.resolve()}** — contains top templates and top tokens.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed40bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d1a5a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Top Real Words (NLTK vocab)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love                 23062\n",
      "ever                 18744\n",
      "life                 16089\n",
      "eva                  14720\n",
      "yahoo                9746\n",
      "baby                 8523\n",
      "may                  7791\n",
      "angel                7092\n",
      "sexy                 6318\n",
      "alex                 5420\n",
      "pink                 4977\n",
      "june                 4977\n",
      "sam                  4760\n",
      "jan                  4684\n",
      "girl                 4568\n",
      "july                 4555\n",
      "you                  4388\n",
      "blue                 4359\n",
      "chris                4325\n",
      "star                 4139\n",
      "red                  4074\n",
      "mike                 4058\n",
      "the                  3975\n",
      "jay                  3835\n",
      "mar                  3785\n",
      "man                  3672\n",
      "ash                  3644\n",
      "april                3629\n",
      "john                 3614\n",
      "rock                 3610\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Top Fragments (non-dictionary)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a                    89281\n",
      "m                    62740\n",
      "s                    57834\n",
      "k                    53087\n",
      "j                    49701\n",
      "b                    48169\n",
      "d                    46586\n",
      "c                    45580\n",
      "r                    45260\n",
      "l                    44758\n",
      "n                    41989\n",
      "t                    38440\n",
      "e                    36806\n",
      "p                    30604\n",
      "com                  30434\n",
      "g                    28065\n",
      "i                    26372\n",
      "h                    26309\n",
      "x                    23275\n",
      "u                    22284\n",
      "f                    21639\n",
      "y                    20971\n",
      "me                   19312\n",
      "o                    19228\n",
      "v                    18234\n",
      "w                    17447\n",
      "z                    14557\n",
      "hotmail              13419\n",
      "q                    8750\n",
      "my                   7087\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Top Real Words (NLTK vocab)\"))\n",
    "for w, c in model.top_tokens(\"WORD\", 30):\n",
    "    print(f\"{w:20} {c}\")\n",
    "\n",
    "display(Markdown(\"### Top Fragments (non-dictionary)\"))\n",
    "for f, c in model.top_tokens(\"FRAG\", 30):\n",
    "    print(f\"{f:20} {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c90431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Trained on 6 templates. Unique templates: 6**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "password => WORD8 ['password'] score: -3.1245651453969594\n",
      "passw0rd => FRAG|DIGITS1|FRAG ['passw', '0', 'rd'] score: -6.56213017122999\n",
      "P@ssw0rd => FRAG|SYMBOL|FRAG|DIGITS1|FRAG ['p', '@', 'ssw', '0', 'rd'] score: -9.424331052159458\n",
      "john1987 => WORD4|DIGITS4 ['john', '1987'] score: -4.7340030578310595\n",
      "superman2020! => WORD8|DIGITS4|SYMBOL ['superman', '2020', '!'] score: -5.650293789705215\n",
      "aaaa1111bbbb => FRAG|DIGITS4|FRAG ['aaaa', '1111', 'bbbb'] score: -7.373060387446318\n",
      "Top WORD tokens: [('password', 1), ('john', 1), ('superman', 1)]\n",
      "Top FRAG tokens: [('rd', 2), ('passw', 1), ('p', 1), ('ssw', 1), ('aaaa', 1), ('bbbb', 1)]\n"
     ]
    }
   ],
   "source": [
    "m = PCFGLite(alpha=1.0, do_leet=True)\n",
    "sample_pw = [\"password\", \"passw0rd\", \"P@ssw0rd\", \"john1987\", \"superman2020!\", \"aaaa1111bbbb\"]\n",
    "m.fit_list(sample_pw, verbose=True)\n",
    "for pw in sample_pw:\n",
    "    toks, tpl = tokenize(pw, do_leet=True)\n",
    "    print(pw, \"=>\", tpl, toks, \"score:\", m.score(pw))\n",
    "print(\"Top WORD tokens:\", m.top_tokens(\"WORD\", 20))\n",
    "print(\"Top FRAG tokens:\", m.top_tokens(\"FRAG\", 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70c783a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_long on passwords from Data-Breach/rockyou.txt with length >= 12 ...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 1000000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Trained on 1500000 passwords..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Trained on 1573606 templates. Unique templates: 25729**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Saved long-password snapshot to **/Users/twochar/vS/Password-Decryption/pcfg_snapshot_long_ge12.json**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total long-password templates: 1573606; unique templates: 25729\n"
     ]
    }
   ],
   "source": [
    "# Create snapshot JSON for passwords with length >= 12\n",
    "MIN_LEN = 12\n",
    "OUT_LONG_SNAP = Path(\"pcfg_snapshot_long_ge12.json\")\n",
    "MAX_LINES = None  # optional: set to an int to limit how many lines to process for testing\n",
    "\n",
    "def iter_lines_minlen(path: str, min_len: int = 12, max_lines: int = None):\n",
    "    \"\"\"Stream lines from `path` yielding only passwords whose length >= min_len.\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with p.open(\"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines and i >= max_lines:\n",
    "                break\n",
    "            pw = line.rstrip(\"\\n\\r\")\n",
    "            if not pw:\n",
    "                continue\n",
    "            if len(pw) >= min_len:\n",
    "                yield pw\n",
    "\n",
    "# Build model trained only on long passwords\n",
    "model_long = PCFGLite(alpha=1.0, do_leet=model.do_leet)\n",
    "\n",
    "# Try to stream from DATA_PATH; if not present, fallback to sample list if available\n",
    "try:\n",
    "    pw_iter = iter_lines_minlen(DATA_PATH, min_len=MIN_LEN, max_lines=MAX_LINES)\n",
    "    # Count filtered items while streaming into model.fit_list -- feed the iterator directly\n",
    "    # fit_list will consume the iterator streaming-style\n",
    "    print(f\"Training model_long on passwords from {DATA_PATH} with length >= {MIN_LEN} ...\")\n",
    "    model_long.fit_list(pw_iter, max_samples=None, verbose=True, use_vocab=True)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Data file not found at {DATA_PATH}: {e}. Falling back to demo sample_pw (if available).\")\n",
    "    try:\n",
    "        long_sample = [pw for pw in sample_pw if len(pw) >= MIN_LEN]\n",
    "    except NameError:\n",
    "        long_sample = []\n",
    "    if not long_sample:\n",
    "        raise RuntimeError(\"No input data available: set DATA_PATH to a valid file or provide sample_pw.\")\n",
    "    model_long.fit_list(long_sample, max_samples=None, verbose=True, use_vocab=True)\n",
    "\n",
    "# Snapshot and save\n",
    "snap_long = model_long.snapshot()\n",
    "# add metadata about filter\n",
    "snap_long[\"filter\"] = {\"min_len\": MIN_LEN}\n",
    "out_text = json.dumps(snap_long)\n",
    "OUT_LONG_SNAP.write_text(out_text, encoding=\"utf8\")\n",
    "display(Markdown(f\"Saved long-password snapshot to **{OUT_LONG_SNAP.resolve()}**\"))\n",
    "print(f\"Total long-password templates: {snap_long['total_templates']}; unique templates: {snap_long['unique_templates']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abf6f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1,000,000 passwords...\n",
      "Processed 2,000,000 passwords...\n",
      "Processed 3,000,000 passwords...\n",
      "Processed 4,000,000 passwords...\n",
      "Processed 5,000,000 passwords...\n",
      "Processed 6,000,000 passwords...\n",
      "Processed 7,000,000 passwords...\n",
      "Processed 8,000,000 passwords...\n",
      "Processed 9,000,000 passwords...\n",
      "Processed 10,000,000 passwords...\n",
      "Processed 11,000,000 passwords...\n",
      "Processed 12,000,000 passwords...\n",
      "Processed 13,000,000 passwords...\n",
      "Processed 14,000,000 passwords...\n",
      "Processed 14,344,390 passwords.\n",
      "Found 5,212,697 unique FRAG tokens (total occurrences: 9,389,113).\n",
      "Wrote JSON: /Users/twochar/vS/Password-Decryption/frag_tokens_all_len_ge3.json\n",
      "Wrote TSV: /Users/twochar/vS/Password-Decryption/frag_tokens_all_len_ge3.tsv\n",
      "\n",
      "Top 50 FRAG tokens (len >= 3):\n",
      "com                  30434\n",
      "hotmail              13419\n",
      "luv                  5556\n",
      "feb                  4080\n",
      "nov                  3993\n",
      "abc                  3934\n",
      "lil                  3906\n",
      "dec                  3814\n",
      "mom                  3547\n",
      "oct                  3371\n",
      "teamo                3180\n",
      "lyf                  2951\n",
      "ryan                 2937\n",
      "jen                  2639\n",
      "aug                  2555\n",
      "ilove                2552\n",
      "pre                  2525\n",
      "nicole               2488\n",
      "rockyou              2341\n",
      "amo                  2153\n",
      "aol                  2105\n",
      "mama                 2097\n",
      "fuck                 2007\n",
      "ashley               1938\n",
      "lyfe                 1892\n",
      "lol                  1879\n",
      "sep                  1874\n",
      "emo                  1796\n",
      "bebe                 1775\n",
      "ali                  1721\n",
      "jojo                 1705\n",
      "nikki                1674\n",
      "xxx                  1670\n",
      "hottie               1669\n",
      "iloveyou             1653\n",
      "nena                 1653\n",
      "hannah               1648\n",
      "babygirl             1586\n",
      "mia                  1585\n",
      "shorty               1510\n",
      "lala                 1496\n",
      "www                  1484\n",
      "steph                1483\n",
      "cody                 1415\n",
      "boys                 1410\n",
      "tweety               1395\n",
      "fer                  1369\n",
      "asd                  1297\n",
      "lauren               1282\n",
      "jenn                 1280\n"
     ]
    }
   ],
   "source": [
    "# Extract FRAG tokens (length >= 3) from ALL passwords in DATA_PATH (streaming)\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MIN_TOKEN_LEN = 3\n",
    "OUT_JSON = Path(\"frag_tokens_all_len_ge3.json\")\n",
    "OUT_TSV = Path(\"frag_tokens_all_len_ge3.tsv\")\n",
    "\n",
    "def iter_all_lines(path: str):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with p.open(\"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip(\"\\n\\r\")\n",
    "\n",
    "frag_counter = Counter()\n",
    "processed = 0\n",
    "\n",
    "for i, pw in enumerate(iter_all_lines(DATA_PATH)):\n",
    "    if not pw:\n",
    "        continue\n",
    "    runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n",
    "    for r in runs:\n",
    "        slot, token_for_counts, _ = classify_run(r, do_leet=False, use_vocab=True)\n",
    "        if slot == \"FRAG\" and isinstance(token_for_counts, str) and len(token_for_counts) >= MIN_TOKEN_LEN:\n",
    "            frag_counter[token_for_counts] += 1\n",
    "    processed += 1\n",
    "    if (i + 1) % 1_000_000 == 0:  # progress print every million\n",
    "        print(f\"Processed {i+1:,} passwords...\")\n",
    "\n",
    "# Results\n",
    "total_occ = sum(frag_counter.values())\n",
    "unique_tokens = len(frag_counter)\n",
    "print(f\"Processed {processed:,} passwords.\")\n",
    "print(f\"Found {unique_tokens:,} unique FRAG tokens (total occurrences: {total_occ:,}).\")\n",
    "\n",
    "# Save JSON\n",
    "OUT_JSON.write_text(json.dumps({\n",
    "    \"N_lines\": processed,\n",
    "    \"min_token_len\": MIN_TOKEN_LEN,\n",
    "    \"total_occurrences\": total_occ,\n",
    "    \"unique_tokens\": unique_tokens,\n",
    "    \"frag_tokens\": frag_counter.most_common()\n",
    "}, ensure_ascii=False))\n",
    "print(\"Wrote JSON:\", OUT_JSON.resolve())\n",
    "\n",
    "# Save TSV\n",
    "with OUT_TSV.open(\"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"token\\tcount\\n\")\n",
    "    for token, cnt in frag_counter.most_common():\n",
    "        f.write(f\"{token}\\t{cnt}\\n\")\n",
    "print(\"Wrote TSV:\", OUT_TSV.resolve())\n",
    "\n",
    "# Quick preview\n",
    "print(\"\\nTop 50 FRAG tokens (len >= 3):\")\n",
    "for token, cnt in frag_counter.most_common(50):\n",
    "    print(f\"{token:20} {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f9bb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mpcfg_model.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     pickle.dump({\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtemplate_counts\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmodel\u001b[49m.template_counts,\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mslot_counts\u001b[39m\u001b[33m\"\u001b[39m: {k: \u001b[38;5;28mdict\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m model.slot_counts.items()},\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtotal_templates\u001b[39m\u001b[33m\"\u001b[39m: model.total_templates,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m: model.alpha,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdo_leet\u001b[39m\u001b[33m\"\u001b[39m: model.do_leet\n\u001b[32m     11\u001b[39m     }, f)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved model to\u001b[39m\u001b[33m\"\u001b[39m, model_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# in other_notebook.ipynb (after training finished)\n",
    "import pickle\n",
    "model_path = \"pcfg_model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"template_counts\": model.template_counts,\n",
    "        \"slot_counts\": {k: dict(v) for k,v in model.slot_counts.items()},\n",
    "        \"total_templates\": model.total_templates,\n",
    "        \"alpha\": model.alpha,\n",
    "        \"do_leet\": model.do_leet\n",
    "    }, f)\n",
    "print(\"Saved model to\", model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

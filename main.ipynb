{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a1d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & config\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Union\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# optional progress bar\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "# reproducibility/logging\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b375ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded English vocab size: 234377\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — NLTK vocab load (safe)\n",
    "import nltk\n",
    "try:\n",
    "    _ = nltk.corpus.words.words()\n",
    "except Exception:\n",
    "    nltk.download(\"words\")\n",
    "\n",
    "from nltk.corpus import words as nltk_words\n",
    "ENGLISH_VOCAB = set(w.lower() for w in nltk_words.words())\n",
    "print(\"Loaded English vocab size:\", len(ENGLISH_VOCAB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0632a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Tokenizer + leet normalization + helpers\n",
    "LEET_MAP = str.maketrans({\n",
    "    '0': 'o', '1': 'l', '3': 'e', '4': 'a', '5': 's', '7': 't', '@': 'a', '$': 's', '!': 'i'\n",
    "})\n",
    "\n",
    "def leet_normalize(s: str) -> str:\n",
    "    return s.translate(LEET_MAP).lower()\n",
    "\n",
    "def classify_run(r: str, do_leet: bool = False, use_vocab: bool = True):\n",
    "    if r.isdigit():\n",
    "        return \"DIGITS\", r, f\"DIGITS{len(r)}\"\n",
    "    if r.isalpha():\n",
    "        token_norm = r.lower()\n",
    "        if do_leet:\n",
    "            token_norm = leet_normalize(token_norm)\n",
    "        if use_vocab and len(token_norm) >= 3 and token_norm in ENGLISH_VOCAB:\n",
    "            return \"WORD\", token_norm, f\"WORD{len(token_norm)}\"\n",
    "        else:\n",
    "            return \"FRAG\", token_norm, \"FRAG\"\n",
    "    return \"SYMBOL\", r, \"SYMBOL\"\n",
    "\n",
    "def tokenize(password: str, do_leet: bool = False, use_vocab: bool = True) -> Tuple[List[str], str]:\n",
    "    pw = password.strip()\n",
    "    runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n",
    "    tokens_for_counts = []\n",
    "    template_parts = []\n",
    "    for r in runs:\n",
    "        slot, token_for_counts, tpl = classify_run(r, do_leet=do_leet, use_vocab=use_vocab)\n",
    "        tokens_for_counts.append(token_for_counts)\n",
    "        template_parts.append(tpl)\n",
    "    template = \"|\".join(template_parts)\n",
    "    return tokens_for_counts, template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a087f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — PCFGLite class (improved, trim helper, streaming fit)\n",
    "class PCFGLite:\n",
    "    def __init__(self, alpha: float = 1.0, do_leet: bool = False):\n",
    "        self.template_counts = Counter()\n",
    "        self.slot_counts = defaultdict(Counter)\n",
    "        self.total_templates = 0\n",
    "        self.alpha = float(alpha)\n",
    "        self.do_leet = do_leet\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PCFGLite(total_templates={self.total_templates}, unique_templates={len(self.template_counts)})\"\n",
    "\n",
    "    def trim_slot_counts(self, top_n=100000):\n",
    "        for s in list(self.slot_counts.keys()):\n",
    "            self.slot_counts[s] = Counter(dict(self.slot_counts[s].most_common(top_n)))\n",
    "\n",
    "    def fit_list(self, pw_list, max_samples: int = None, verbose: bool = True, use_vocab: bool = True, trim_top_n: int = None):\n",
    "        for i, pw in enumerate(pw_list):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            if not pw:\n",
    "                continue\n",
    "            tokens, template = tokenize(pw, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "            self.template_counts[template] += 1\n",
    "            self.total_templates += 1\n",
    "\n",
    "            runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n",
    "            for r in runs:\n",
    "                slot, token_for_counts, _ = classify_run(r, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "                self.slot_counts[slot][token_for_counts] += 1\n",
    "\n",
    "            \n",
    "\n",
    "            if trim_top_n and (i + 1) % 500000 == 0:\n",
    "                for s in list(self.slot_counts.keys()):\n",
    "                    self.slot_counts[s] = Counter(dict(self.slot_counts[s].most_common(trim_top_n)))\n",
    "\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**Trained on {self.total_templates} templates. Unique templates: {len(self.template_counts)}**\"))\n",
    "\n",
    "    def fit_file(self, filepath: str, max_lines: int = None, use_vocab: bool = True, show_progress: bool = True, trim_top_n: int = None):\n",
    "        p = Path(filepath)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(filepath)\n",
    "\n",
    "        def iter_lines():\n",
    "            with p.open(\"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if max_lines and i >= max_lines:\n",
    "                        break\n",
    "                    yield line.rstrip(\"\\n\\r\")\n",
    "\n",
    "        it = iter_lines()\n",
    "        if show_progress and tqdm is not None and max_lines is None:\n",
    "            it = tqdm(it, desc=f\"Reading {p.name}\")\n",
    "        self.fit_list(it, max_samples=None, verbose=True, use_vocab=use_vocab, trim_top_n=trim_top_n)\n",
    "\n",
    "    def template_prob(self, template: str) -> float:\n",
    "        V = len(self.template_counts)\n",
    "        return (self.template_counts[template] + self.alpha) / (self.total_templates + self.alpha * (V + 1))\n",
    "\n",
    "    def slot_token_prob(self, slot_type: str, token: str) -> float:\n",
    "        counter = self.slot_counts.get(slot_type, Counter())\n",
    "        total = sum(counter.values())\n",
    "        V = len(counter)\n",
    "        return (counter[token] + self.alpha) / (total + self.alpha * (V + 1))\n",
    "\n",
    "    def score(self, password: str, use_vocab: bool = True) -> float:\n",
    "        tokens, template = tokenize(password, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "        logp = math.log(self.template_prob(template))\n",
    "        runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', password)\n",
    "        for r in runs:\n",
    "            slot, token_for_counts, _ = classify_run(r, do_leet=self.do_leet, use_vocab=use_vocab)\n",
    "            p = self.slot_token_prob(slot, token_for_counts)\n",
    "            logp += math.log(p)\n",
    "        return logp\n",
    "\n",
    "    def top_templates(self, n=30):\n",
    "        return self.template_counts.most_common(n)\n",
    "\n",
    "    def top_tokens(self, slot_type: str, n=30):\n",
    "        return self.slot_counts.get(slot_type, Counter()).most_common(n)\n",
    "\n",
    "    def snapshot(self, top_templates_n=200, top_words_n=500, top_digits_n=200):\n",
    "        return {\n",
    "            \"total_templates\": self.total_templates,\n",
    "            \"unique_templates\": len(self.template_counts),\n",
    "            \"top_templates\": self.top_templates(top_templates_n),\n",
    "            \"top_words\": self.top_tokens(\"WORD\", top_words_n),\n",
    "            \"top_digits\": self.top_tokens(\"DIGITS\", top_digits_n),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008d75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Save / Load helpers (state-only by default)\n",
    "def save_model(path: Union[str, Path], model: PCFGLite, state_only: bool = True):\n",
    "    p = Path(path)\n",
    "    if state_only:\n",
    "        data = {\n",
    "            \"template_counts\": dict(model.template_counts),\n",
    "            \"slot_counts\": {k: dict(v) for k, v in model.slot_counts.items()},\n",
    "            \"total_templates\": int(model.total_templates),\n",
    "            \"alpha\": float(model.alpha),\n",
    "            \"do_leet\": bool(model.do_leet),\n",
    "        }\n",
    "        with p.open(\"wb\") as f:\n",
    "            pickle.dump({\"__pcfg_state_v1\": True, \"data\": data}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with p.open(\"wb\") as f:\n",
    "            pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    logging.info(f\"Saved model to {p.resolve()} (state_only={state_only})\")\n",
    "\n",
    "def load_model(path: Union[str, Path], state_only: bool = True) -> PCFGLite:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    with p.open(\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    if state_only:\n",
    "        if isinstance(data, dict) and data.get(\"__pcfg_state_v1\"):\n",
    "            st = data[\"data\"]\n",
    "        else:\n",
    "            st = data\n",
    "        model = PCFGLite(alpha=st.get(\"alpha\", 1.0), do_leet=st.get(\"do_leet\", False))\n",
    "        model.template_counts = Counter(st.get(\"template_counts\", {}))\n",
    "        model.slot_counts = defaultdict(Counter, {k: Counter(v) for k, v in st.get(\"slot_counts\", {}).items()})\n",
    "        model.total_templates = int(st.get(\"total_templates\", 0))\n",
    "        return model\n",
    "    else:\n",
    "        obj = data\n",
    "        if not isinstance(obj, PCFGLite):\n",
    "            raise TypeError(\"Unpickled object is not a PCFGLite instance.\")\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427ad5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Trained on 14344390 templates. Unique templates: 29295**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to /Users/twochar/vS/Password-Decryption/pcfg_model_all_state.pkl (state_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top templates: [('FRAG', 4051390), ('FRAG|DIGITS2', 1440243), ('FRAG|DIGITS1', 886830), ('FRAG|DIGITS4', 755828), ('FRAG|DIGITS3', 515233)]\n",
      "Top words: [('love', 23062), ('ever', 18744), ('life', 16089), ('eva', 14720), ('yahoo', 9746), ('baby', 8523), ('may', 7791), ('angel', 7092), ('sexy', 6318), ('alex', 5420)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Demo training & save (small sample fallback)\n",
    "DATA_PATH = Path(\"Data-Breach/rockyou.txt\")\n",
    "sample_pw = [\"password\", \"passw0rd\", \"P@ssw0rd\", \"john1987\", \"superman2020!\", \"aaaa1111bbbb\"]\n",
    "\n",
    "# Train model on either large file or sample\n",
    "model = PCFGLite(alpha=1.0, do_leet=True)\n",
    "if DATA_PATH.exists():\n",
    "    # logging.info(f\"Training model on {DATA_PATH} (full). This may take a while.\")\n",
    "    model.fit_file(str(DATA_PATH), max_lines=None, use_vocab=True, show_progress=False, trim_top_n=250000)\n",
    "else:\n",
    "    logging.info(\"DATA_PATH not found; training on small sample_pw for demo.\")\n",
    "    model.fit_list(sample_pw, verbose=False)\n",
    "\n",
    "# Save state-only after training\n",
    "save_model(\"pcfg_model_all_state.pkl\", model, state_only=True)\n",
    "\n",
    "# quick inspect\n",
    "print(\"Top templates:\", model.top_templates(5))\n",
    "print(\"Top words:\", model.top_tokens(\"WORD\", 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469449c1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7 — Build model for passwords >= 6 chars, save\nMIN_LEN = 6\nmodel_ge6 = PCFGLite(alpha=1.0, do_leet=True)\n\ndef iter_lines_minlen(path: Union[str, Path], min_len: int = 6, max_lines: int = None):\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(path)\n    with p.open(\"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n        for i, line in enumerate(f):\n            if max_lines and i >= max_lines:\n                break\n            pw = line.rstrip(\"\\n\\r\")\n            if not pw:\n                continue\n            if len(pw) >= min_len:\n                yield pw\n\nif DATA_PATH.exists():\n    model_ge6.fit_list(iter_lines_minlen(DATA_PATH, min_len=MIN_LEN), max_samples=None, verbose=True, use_vocab=True, trim_top_n=250000)\nelse:\n    model_ge6.fit_list([pw for pw in sample_pw if len(pw) >= MIN_LEN], verbose=True)\n\nsave_model(\"pcfg_model_ge6_state.pkl\", model_ge6, state_only=True)\nprint(\"Saved ge6 model snapshot. Total ge6 templates:\", model_ge6.total_templates)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df373a8",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8 — FRAG extraction (streaming) -> TSVs (all + len >=6)\nMIN_TOKEN_LEN = 3\nMIN_PW_LEN = 6\nTOP_K_FRAGS = 200000\n\nOUT_TSV_ALL = Path(\"frag_tokens_all.tsv\")\nOUT_TSV_SHORT = Path(\"frag_tokens_len6.tsv\")\n\nfrag_counter_all = Counter()\nfrag_counter_short = Counter()\n\nif DATA_PATH.exists():\n    it = (line.rstrip(\"\\n\\r\") for line in DATA_PATH.open(\"r\", encoding=\"latin-1\", errors=\"ignore\"))\nelse:\n    it = iter(sample_pw)\n\nfor i, pw in enumerate(it):\n    if not pw:\n        continue\n\n    runs = re.findall(r'[A-Za-z]+|\\d+|[^A-Za-z\\d]+', pw)\n\n    # Collect for \"all\"\n    for r in runs:\n        slot, token_for_counts, _ = classify_run(r, do_leet=model.do_leet, use_vocab=True)\n        if slot == \"FRAG\" and isinstance(token_for_counts, str) and len(token_for_counts) >= MIN_TOKEN_LEN:\n            frag_counter_all[token_for_counts] += 1\n            # Also collect for \"short\" if pw is >= 6\n            if len(pw) >= MIN_PW_LEN:\n                frag_counter_short[token_for_counts] += 1\n\n    if (i + 1) % 1_000_000 == 0:\n        logging.info(f\"Processed {i+1:,} passwords...\")\n\n# Write ALL passwords TSV\nwith OUT_TSV_ALL.open(\"w\", encoding=\"utf8\") as f:\n    f.write(\"token\\tcount\\n\")\n    for token, cnt in frag_counter_all.most_common(TOP_K_FRAGS):\n        f.write(f\"{token}\\t{cnt}\\n\")\nlogging.info(f\"Wrote frag TSV (all, top {TOP_K_FRAGS}) to {OUT_TSV_ALL.resolve()}\")\n\n# Write SHORT (>=6) passwords TSV\nwith OUT_TSV_SHORT.open(\"w\", encoding=\"utf8\") as f:\n    f.write(\"token\\tcount\\n\")\n    for token, cnt in frag_counter_short.most_common(TOP_K_FRAGS):\n        f.write(f\"{token}\\t{cnt}\\n\")\nlogging.info(f\"Wrote frag TSV (len >=6, top {TOP_K_FRAGS}) to {OUT_TSV_SHORT.resolve()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529fb14",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9 — Snapshot save (bounded sizes)\nsnap = model.snapshot(top_templates_n=1000, top_words_n=2000, top_digits_n=500)\nPath(\"pcfg_all.json\").write_text(json.dumps(snap, ensure_ascii=False))\nsnap_ge6 = model_ge6.snapshot(top_templates_n=1000, top_words_n=2000, top_digits_n=500)\nsnap_ge6[\"filter\"] = {\"min_len\": MIN_LEN}\nPath(\"pcfg_of_len6_or_more.json\").write_text(json.dumps(snap_ge6, ensure_ascii=False))\nlogging.info(\"Saved snapshots.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dedf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to /Users/twochar/vS/Password-Decryption/tmp_state.pkl (state_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roundtrip OK. Example score: -52.39906723052136\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Quick smoke-test (save -> load -> score)\n",
    "save_model(\"tmp_state.pkl\", model, state_only=True)\n",
    "model2 = load_model(\"tmp_state.pkl\", state_only=True)\n",
    "assert isinstance(model2, PCFGLite)\n",
    "print(\"Roundtrip OK. Example score:\", model2.score(\"P@ssw0rd123!\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}